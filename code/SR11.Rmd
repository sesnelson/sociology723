---
title: 'Statistical Rethinking Ch. 11: Binomial and Poisson Regression'
author: "Samuel Snelson"
date: "`r Sys.Date()`"
output: 
  html_document:
    theme: paper
    df_print: kable
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rethinking)

# fitting models
library(rstanarm)

# for getting fitted values from models
library(ggeffects)

theme_set(theme_light(base_family = "Avenir"))
```

## E1

**If an event has probability 0.35, what are the log-odds of this event?** 

Log-odds are defined as the following.

$$
\text{log-odds} = \log \left( \frac{p}{1-p} \right) 
$$
With a probability of 0.32, the log-odds of that event are -0.62. 

$$
\log\left( \frac{0.35}{0.65} \right) = -0.62
$$


## E2

**If an event has log-odds 3.2, what is the probability of this event? **

To convert from log-odds to probability, we can use either of the following functions (both variants of the sigmoid function). 

$$
\frac{e^{\text{log-odds}}}{1 + e^{\text{log-odds}}} = \frac{1}{1 + e^{- \text{log-odds}}}
$$
With a log-odds of 3.2, the probability of this event occuring is 0.96. As a general heuristic, a log-odds of -3 is almost certain to not occur, 0 is equally likely to and not to occur, and 3 is almost certain to occur. 

$$
\frac{e^{3.2}}{1 + e^{3.2}} = 0.96
$$

## E3

**Suppose that a coefficient in a logistic regression has value 1.7. What does this imply about the proportional change in odds of the outcome? **

If a coefficient in a logit model was 1.7, this would imply that for a 1-unit change in the variable corresponding to the coefficient, the model predicts a increase in the log-odds of the outcome by 1.7. Proportionally speaking, the outcome in its original scale would increase by a factor of $e^{1.7}$.  

$$
\begin{align}
y_{\text{log-odds}} &= \alpha+ 1.7x \\
y_{\text{odds}} &= e^{\alpha + 1.7x} \\ 
&= e^{\alpha} \times e^{1.7x}
\end{align}
$$


## E4

**Why do Poisson regressions sometimes require the use of an offset? Provide an example. ** 

An offset, in the context of Poisson regression models, refers to the manipulation of the time scaling of an rate-outcome. Recall that Poisson distributions are defined by a single parameter, $\lambda$, which represents either the expected value (noting the assumed equivalence of the mean and variance) or the rate of change of the outcome (with respect to some measure of change like time). Rates can be proportionately scaled up or down (e.g., 3 meals/day is equivalent with 21 meals/week). Differently scaled rates cannot be directly compared because, if one does, R will try to compare 3 and 21 rather than regard them as equivalent. Using an offset in Poisson regression models resolves this issue and regards the rates as equivalent by incorporating the logarithm of the inverse of the time scale with respect to the other rate scales. For example, the offset for the meals/day scale could be increased by a factor of 7 to equate to the meals/week scale. Conversely, meals/week could be reduced by a factor of 7 to equate to meals/day. 


## M1

**As explained in the chapter, binomial data can be organized in aggregated and disaggregated forms, without any impact on inference. But the likelihood of the data does change when the data are converted between the two formats. Can you explain why?**

With disaggregated and aggregated binomial regression, inference is equivalent because the parameters corresponding to the same variables and levels are the same. As in a different case, using indeces or a reference category for a categorical variable makes no difference on the interpretation of a logit model. 

However, the computation of likelihood depends on the form of the observations in a dataset. In the aggregated model, observations are more influential (for WAIC and PSIS) because each observation is a grouping of multiple observations. 


## M2

**If a coefficient in a Poisson regression has value 1.7, what does this imply about the change in the outcome?**


The standard Poisson model with a linear function mapped onto $\lambda$ is as follows. 

$$ 
\begin{align}
Y_i \sim &\ \text{Poisson}(\lambda_i) \\
\log(\lambda_i) = &\ \alpha + \sum \beta x_i 
\end{align}
$$

If a coefficient in this model is 1.7, then the model predicts that the outcome, on the log scale, is expected to increase by 1.7. Converted to its original scaling, the outcome is expected to increase by a factor of $e^{1.7}$. 

Just to quickly demonstrate this we can look at how the log link effects the coefficient interpretation with some fake coefficients ($\alpha = 10$ and $\beta = 1.7$ as we've been told).

$$
\begin{align}
Y_i \sim &\ \text{Poisson}(\lambda_i) \\
\log(\lambda_i) = &\ 10 + 1.7x \\ 
e^{\log(\lambda_i)} = &\ e^{10 + 1.7x} \\
\lambda_i = & \ e^{10 + 1.7x} \\
= & \ e^{10} \times e^{1.7x} 
\end{align}
$$

## M4

**Explain why the logit link is appropriate for a binomial generalized linear model.**

The logit link is appropriate for binomial glms (like logistic regression) because it allows for a binary outcome to be modelled with a linear function. 

I like to think of this visually, where we have some binary outcome data. 

```{r}
# Simulating continuous x binary relationship with positive correlation
dem1 <- tibble(x = seq(1, 200, by = 1), 
               y = c(rbinom(100, 1, 0.1), rbinom(100, 1, 0.9)))

cor(dem1$x, dem1$y)

ggplot(dem1, aes(x, y)) + 
  geom_point() + 
  geom_smooth(method = "lm", 
              se = FALSE)
```

We have the issue where the model makes predicts outside the space of possible values the outcome can take on ($y \in \{0,1 \}$ while $\hat{y} \in (-\infty, \infty)$). Economists and others, however, have developed robustness strategies to make this approach useful (the Linear Probability Model, where predicted values represent the probability of the outcome occuring). 

So where do we go from here. We want to keep the conveniences of the linear model while modeling probability! The logit link involves two steps - converting the outcome variable into the odds of the outcome variable and taking the natural log of the odds. This transforms the outcome space from just 0 and 1 to from $-\infty$ to $\infty$. This allows us to model the log odds of the outcome with a linear function! 

$$
\begin{align}
y_i &= \alpha + \beta x \ | \ y \in \{0,1 \} \\ 
y_{\text{odds}} &= \frac{P(y = 1)}{P(y = 0)} \ | \ y_{\text{odds}} \in [0, \infty) \\ 
y_{\text{log-odds}} &= \log \left( \frac{P(y = 1)}{P(y = 0)}\right) \ | \ y_{\text{log-odds}} \in (-\infty, \infty)
\end{align}
$$

To follow through with the visualization, let's generate the logistic regression model and see how it looks. 

```{r, results = FALSE}
m <- stan_glm(y ~ x, 
               data = dem1, 
               family = binomial(link = "logit"))

```

```{r, results = FALSE}
pred2 <- ggpredict(m, terms = "x")

```

```{r}

ggplot(pred2) + 
  geom_point(data = dem1, 
             aes(x, y)) + 
  geom_line(aes(x, predicted)) + 
  geom_ribbon(aes(x, 
                  ymin = conf.low, 
                  ymax = conf.high), 
              fill = "steelblue", 
              alpha = 0.2) + 
  scale_y_continuous(breaks = c(0,1))


```

Wait, what's going on here? What was all that about mapping on a linear function? That's right, we did map on a linear function when we estimated the model! This visualization shows the predicted probabilities of the outcome. The model generated coefficients and predictions in the log-odds scale (this isn't super intuitive, so they are often converted back to probability (using the sigmoid function I talked about earlier)). 

One last reason I didn't mention thus far is that when we model things logarithmically, there is an implicit assertion that the relationship has diminishing returns (After an event has reached 50% probability, increases in the predictor increases the probability less and less). 


## M4

**Explain why the log link is appropriate for a Poisson generalized linear model.** 

Without going into as much detail as the last example, the log link provides two main benefits for Poisson glms - constraining $\lambda$ to be positive (which is required of the expected value of a count outcome variable) and the logarithm describes relationships with diminishing returns (since the Poisson distribution has lower probability relative to the binomial)


## M5

**What would it imply to use a logit link for the mean of a Poisson generalized linear model? Can you think of a real research problem for which this would make sense?**

Using the logit link for a Poisson glm would imply the conversion of the mean into the log-odds of some event occuring. I am having a bit of trouble conceptualizing whether or not this step requires the dichotomization of the outcome (in terms of greater or less than some value). I imagine this may be relevant if we were curious whether, for some distribution of wait times (at a hospital, for example), wait times were more likely to be more or less than a certain time. 


## M6

**State the constraints for which the binomial and Poisson distributions have maximum entropy. Are the constraints different at all for binomial and Poisson? Why or why not?** 


I'll state first that the maximum entropy constraints for binomial and Poisson distributions are the same because the Poisson distribution is just a special case of the binomial when there are a large number of counts (often with an unknown theoretical maximum) with low probability of occurrence. 

The binomial distribution is a maximum entropy distribution when all one is willing to assume about a variable is that it is binary and has a constant expected value (take a coin for example, with an expected value of either outcome of 0.5). To say that the Poisson distribution is a maximum entropy distribution is equivalent to saying the binomial distribution is as well since the Poisson distribution is a type of binomial distribution. 


## M7

**Use quap to construct a quadratic approximate posterior distribution for the chimpanzee model that includes a unique intercept for each actor, m11.4 (page 330). Compare the quadratic approximation to the posterior distribution produced instead from MCMC. Can you explain both the differences and the similarities between the approximate and the MCMC distributions? Relax the prior on the actor intercepts to Normal(0,10). Re-estimate the posterior using both ulam and quap. Do the differences increase or decrease? Why?** 

```{r}
data(chimpanzees)

d <- chimpanzees %>% 
  # treatment variable captures all 4 treatment levels
  mutate(treatment = 1 + prosoc_left + (2 * condition)) %>% 
  select(actor, treatment, pulled_left)

# 1: prosocial on right & no partner
# 2: prosocial on left & no partner
# 3: prosocial on right & partner
# 4: prosocial on left & partner
```

Writing functions to extract samples and visualize parameters in outcome scale later on. 

```{r}

post_estim <- \(m) {
  
  sample <- extract.samples(m) %>% 
    as.data.frame() %>% 
    pivot_longer(cols = everything(), 
                 names_to = "term",
                 values_to = "estimate") %>% 
    mutate(estimate = inv_logit(estimate),
           type = if_else(str_detect(term, "a"), "alpha", "beta"))
  
  return(sample)
}


post_viz <- \(d, p, estim, cap) {
  
  title = if_else(p == "alpha", 
                  "Individual Chimp Preferences for Pulling Left", 
                  "Treatment Effects for Pulling Left")
  
  subtitle = if_else(estim == "quap", 
                     "with quadratic approximation", 
                     "with MCMC estimation")
  
  caption = if_else(cap == "NA", "", "Note: actor prior of Normal(0, 10)")
  
  d %>% 
    filter(type == p) %>% 
    ggplot(aes(reorder(term, -estimate), estimate)) + 
    geom_boxplot() + 
    coord_flip() + 
    labs(title = title,
         subtitle = subtitle,
         caption = caption,
         x = "chimp", 
         y = "estimate")
}

```


Estimating `quap()` and `ulam()` models with indexed actor and treatment variables 

Passing to quadratic approximator 

```{r}
m2 <- quap(
  alist(pulled_left ~ dbinom(1, p), 
        logit(p) <- a[actor] + b[treatment],
        a[actor] ~ dnorm(0, 1.5),
        b[treatment] ~ dnorm(0, 0.5)),
  data = d)

```

Looking at outcome scale - likelihood of pulling left. 

Recall the treatment leveling: 

1. prosocial on right & no partner
2. prosocial on left & no partner
3. prosocial on right & partner
4. prosocial on left & partner



```{r}

# extracting samples 
post1 <- post_estim(m2)

# Visualizing alphas
post_viz(post1, "alpha", "quap", "NA")

# Visualizing betas
post_viz(post1, "beta", "quap", "NA")

```


MCMC estimation with (Mr.) ulam

```{r, results = FALSE}
m3 <- ulam(
  alist(pulled_left ~ dbinom(1, p), 
        logit(p) <- a[actor] + b[treatment],
        a[actor] ~ dnorm(0, 1.5),
        b[treatment] ~ dnorm(0, 0.5)),
  data = d, 
  chains = 4, 
  log_lik = TRUE)
```


```{r}
#extracting samples
post2 <- post_estim(m3)

# Visualizing alphas with mcmc
post_viz(post2, "alpha", "mcmc", "NA")

# Visualizing betas with mcmc
post_viz(post2, "beta", "mcmc", "NA")
```

The moral of the story here is that quadratic approximation and MCMC estimation generated basically the same exact estimates! We have three chimps who are more prone to pulling the left lever and there is a marginal effect of pulling left when the left option is prosocial (meaning there is food on both sides of the table). However, the effect is basically the same whether or not there is another chimp on the other side of the table, so this is not really evidence for the prosocial argument at all. 


Re-estimating with relaxed actor prior with quap and ulam.

```{r}
# Reffited quap 
m4 <- quap(
  alist(pulled_left ~ dbinom(1, p), 
        logit(p) <- a[actor] + b[treatment],
        a[actor] ~ dnorm(0, 10),
        b[treatment] ~ dnorm(0, 0.5)),
  data = d)
```

```{r, results = FALSE}
# refitted ulam for mcmc
m5 <- ulam(
  alist(pulled_left ~ dbinom(1, p), 
        logit(p) <- a[actor] + b[treatment], 
        a[actor] ~ dnorm(0, 10), 
        b[treatment] ~ dnorm(0, 0.5)), 
  data = d, 
  chains = 4, 
  log_lik = TRUE)
```

Visualizing effects with relaxed prior - first the quap model then mcmc via ulam. 

```{r}
# Samples from quap model
post3 <- post_estim(m4)

# Vizualizing alphas with quap and relaxed prior
post_viz(post3, "alpha", "quap", "relax")

# Visualizing betas with quap and relaxed prior 
post_viz(post3, "beta", "quap", "relax")
```

```{r}
# samples from ulam model 
post4 <- post_estim(m5)

# Visualizing alphas with mcmc and relaxed prior 
post_viz(post4, "alpha", "mcmc", "relax")

# Visualizing betas with mcmc annd relaxed prior 
post_viz(post4, "beta", "mcmc", "relax")
```

Even with the relaxed priors, the results are basically the same.


## M8

**Revisit the data(Kline) islands example. This time drop Hawaii from the sample and refit the models. What changes do you observe?**

First, we'll create separate datasets to feed the models with and without Hawaii. 

```{r}

data(Kline)

d2 <- Kline %>% 
  mutate(logpop = log(population), 
         logpop_std = (logpop - mean(logpop)) / sd(logpop)) %>% 
  select("tools" = total_tools, logpop_std, contact)

d2_drop <- Kline %>% 
  mutate(logpop = log(population), 
         logpop_std = (logpop - mean(logpop)) / sd(logpop)) %>% 
  filter(culture != "Hawaii") %>% 
  select("tools" = total_tools, logpop_std, contact)

```

```{r, results = FALSE}

# Model with Hawaii 
m6 <- stan_glm(tools ~ logpop_std * contact, 
              data = d2, 
              family = poisson(link = "log"), 
              prior_intercept = normal(3, 0.5), 
              prior = normal(0, 0.2))

# Model without Hawaii 
m7 <- stan_glm(tools ~ logpop_std * contact, 
              data = d2_drop, 
              family = poisson(link = "log"), 
              prior_intercept = normal(3, 0.5), 
              prior = normal(0, 0.2))

```

Now let's get predicted values for each model and plot both to see how Hawaii affects the relationships! 

```{r}
pred1 <- ggpredict(m6, terms = c("logpop_std", "contact")) 
pred2 <- ggpredict(m7, terms = c("logpop_std", "contact"))

# Function to visualize the relationship
pred_viz <- \(d, pred, h) {
  
  ggplot(pred) + 
    geom_point(data = d, 
               aes(logpop_std, tools, 
                   color = contact)) + 
    geom_line(aes(x, predicted, 
                  color = group)) + 
    geom_ribbon(aes(x, 
                    ymin = conf.low, 
                    ymax = conf.high, 
                    fill = group), 
                alpha = 0.2)
  
}

# Visualizing population and tools Poisson interaction model (with Hawaii)
pred_viz(d2, pred1)

# Visualizing population and tools Poisson interaction model (without Hawaii) 
pred_viz(d2_drop, pred2)


```

Comparing the plots with and without Hawaii side by side, it seems that Hawaii was the reason the low contact estimates of tools increased rapidy and overtook the high contact estimates - just based on Hawaii's influence. If we disregard Hawaii, the slopes look relatively parallel (even in the interaction model). 













